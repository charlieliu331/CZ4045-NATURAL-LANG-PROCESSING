{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66113b5b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b271945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\scse-cil\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\scse-cil\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\scse-cil\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\scse-cil\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->tensorflow-addons) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import initializers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ceb7f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce45ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/labelled_sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f70dd",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fec918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c407fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scse-cil\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Setting label = 1 for all subjective data\n",
    "for i in range(len(df['Label'])):\n",
    "    if df['Label'][i] == -1:\n",
    "        df['Label'][i] = 1\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90540241",
   "metadata": {},
   "source": [
    "### Making class weights equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1075cc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    759\n",
       "0    369\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09626629",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df[df['Label'] == 1]\n",
    "neut = df[df['Label'] == 0]\n",
    "l = df['Label'].value_counts()[0]\n",
    "df = pd.concat([pos.head(l), neut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63187a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    369\n",
       "0    369\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab255571",
   "metadata": {},
   "source": [
    "0 = Neutral\n",
    "\n",
    "1 = Opinionated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c166eb9",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d16d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred, threshold = 0.5):\n",
    "#     y_pred = tf.convert_to_tensor(y_pred)\n",
    "#     threshold = tf.cast(threshold, y_pred.dtype)\n",
    "#     y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred, threshold = 0.5):\n",
    "#     y_pred = tf.convert_to_tensor(y_pred)\n",
    "#     threshold = tf.cast(threshold, y_pred.dtype)\n",
    "#     y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred, threshold = 0.5):\n",
    "#     y_pred = tf.convert_to_tensor(y_pred)\n",
    "#     threshold = tf.cast(threshold, y_pred.dtype)\n",
    "#     y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c662c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [tf.metrics.BinaryAccuracy(), f1_m, precision_m, recall_m]\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_f1_m', patience = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5ff48",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f842a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Lemmatized_Text'], df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0db2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "    'text': X_train,\n",
    "    'target':y_train\n",
    "}\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "\n",
    "test_dict = {\n",
    "    'text': X_test,\n",
    "    'target':y_test\n",
    "}\n",
    "test_df = pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0510719",
   "metadata": {},
   "source": [
    "### Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277ab6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "map_name_to_handle = {'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'}\n",
    "map_model_to_preprocess = {'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'}\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb62dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157069f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = bert_preprocess_model(train_df['text'])['input_word_ids']\n",
    "x_test = bert_preprocess_model(test_df['text'])['input_word_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d783083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(29593, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "high = max(x_test[0])\n",
    "for i in x_train:\n",
    "    if max(i) > high:\n",
    "        high = max(i)\n",
    "\n",
    "for i in x_test:\n",
    "    if max(i) > high:\n",
    "        high = max(i)\n",
    "print(high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca68fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique_words = high + 1 # cut texts after this number of words\n",
    "maxlen = 256\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e64c73",
   "metadata": {},
   "source": [
    "### Padding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827e20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "y = np.array(train_df['target']).reshape((-1,1))\n",
    "\n",
    "y_test = np.array(test_df['target']).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16f6f3",
   "metadata": {},
   "source": [
    "## Defining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a9c69",
   "metadata": {},
   "source": [
    "### 1. Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43036d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c50ccf",
   "metadata": {},
   "source": [
    "### Define Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ee93776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\n",
    "        super(attention,self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5268f66",
   "metadata": {},
   "source": [
    "### 2. Attention based Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5bc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Att_Bi_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_unique_words, 64, input_length=maxlen))\n",
    "    model.add(Bidirectional(LSTM(32,return_sequences=True)))\n",
    "    model.add(attention(return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504da1c",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "231df9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"weights/att_subjectivity.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d0e40",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2804a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 256, 128)          3788032   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,886,977\n",
      "Trainable params: 3,886,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "10/10 [==============================] - 4s 221ms/step - loss: 0.6932 - binary_accuracy: 0.5051 - f1_m: 0.5715 - precision_m: 0.4725 - recall_m: 0.8007 - val_loss: 0.6911 - val_binary_accuracy: 0.5898 - val_f1_m: 0.5915 - val_precision_m: 0.6218 - val_recall_m: 0.5743\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 0.6696 - binary_accuracy: 0.8746 - f1_m: 0.8853 - precision_m: 0.8582 - recall_m: 0.9267 - val_loss: 0.6725 - val_binary_accuracy: 0.6610 - val_f1_m: 0.6256 - val_precision_m: 0.7251 - val_recall_m: 0.5634\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.5997 - binary_accuracy: 0.7288 - f1_m: 0.6107 - precision_m: 0.7842 - recall_m: 0.5163 - val_loss: 0.7011 - val_binary_accuracy: 0.5661 - val_f1_m: 0.2335 - val_precision_m: 0.5411 - val_recall_m: 0.1586\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 2s 157ms/step - loss: 0.4207 - binary_accuracy: 0.8712 - f1_m: 0.8713 - precision_m: 0.8821 - recall_m: 0.8827 - val_loss: 0.6473 - val_binary_accuracy: 0.6475 - val_f1_m: 0.6440 - val_precision_m: 0.6782 - val_recall_m: 0.6266\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 2s 159ms/step - loss: 0.2999 - binary_accuracy: 0.9627 - f1_m: 0.9643 - precision_m: 0.9698 - recall_m: 0.9632 - val_loss: 0.7558 - val_binary_accuracy: 0.6203 - val_f1_m: 0.5624 - val_precision_m: 0.6737 - val_recall_m: 0.5061\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 2s 162ms/step - loss: 0.8094 - binary_accuracy: 0.7458 - f1_m: 0.8273 - precision_m: 0.7365 - recall_m: 0.9941 - val_loss: 0.7537 - val_binary_accuracy: 0.5458 - val_f1_m: 0.2704 - val_precision_m: 0.6417 - val_recall_m: 0.1791\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 2s 164ms/step - loss: 0.3343 - binary_accuracy: 0.9254 - f1_m: 0.8934 - precision_m: 1.0000 - recall_m: 0.8170 - val_loss: 0.7158 - val_binary_accuracy: 0.5525 - val_f1_m: 0.3424 - val_precision_m: 0.6614 - val_recall_m: 0.2418\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 2s 166ms/step - loss: 0.2457 - binary_accuracy: 0.9729 - f1_m: 0.9707 - precision_m: 0.9731 - recall_m: 0.9694 - val_loss: 0.6948 - val_binary_accuracy: 0.5932 - val_f1_m: 0.5576 - val_precision_m: 0.6429 - val_recall_m: 0.5014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b971a4630>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_LSTM()\n",
    "model.summary()\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.5, random_state=22)\n",
    "model.fit(X_train, y_train, epochs=8, validation_data = [X_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0814af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6281 - binary_accuracy: 0.6216 - f1_m: 0.6119 - precision_m: 0.6800 - recall_m: 0.5705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6280935406684875,\n",
       " 0.6216216087341309,\n",
       " 0.611852765083313,\n",
       " 0.6799784302711487,\n",
       " 0.5704761743545532]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc985d0",
   "metadata": {},
   "source": [
    "### Attention based Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2451f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03da5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 256, 64)           1894016   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256, 64)          24832     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " attention (attention)       (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,919,233\n",
      "Trainable params: 1,919,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scse-cil\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 828ms/step - loss: 0.6946 - binary_accuracy: 0.5327 - f1_m: 0.0545 - precision_m: 0.0375 - recall_m: 0.1000 - val_loss: 0.6932 - val_binary_accuracy: 0.5017 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/12\n",
      "10/10 [==============================] - 8s 800ms/step - loss: 0.6914 - binary_accuracy: 0.5390 - f1_m: 0.1383 - precision_m: 0.2378 - recall_m: 0.1288 - val_loss: 0.6925 - val_binary_accuracy: 0.6068 - val_f1_m: 0.6810 - val_precision_m: 0.5691 - val_recall_m: 0.8703\n",
      "Epoch 3/12\n",
      "10/10 [==============================] - 8s 805ms/step - loss: 0.6894 - binary_accuracy: 0.7864 - f1_m: 0.8233 - precision_m: 0.7055 - recall_m: 0.9944 - val_loss: 0.6918 - val_binary_accuracy: 0.6068 - val_f1_m: 0.6813 - val_precision_m: 0.5687 - val_recall_m: 0.8703\n",
      "Epoch 4/12\n",
      "10/10 [==============================] - 8s 801ms/step - loss: 0.6847 - binary_accuracy: 0.7864 - f1_m: 0.8042 - precision_m: 0.6812 - recall_m: 1.0000 - val_loss: 0.6899 - val_binary_accuracy: 0.6068 - val_f1_m: 0.6647 - val_precision_m: 0.5660 - val_recall_m: 0.8205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b97e0cd68>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_Att_Bi_LSTM()\n",
    "model.summary()\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.5, random_state=15)\n",
    "model.fit(X_train, y_train, epochs=12, validation_data = [X_val, y_val], callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aab39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6895 - binary_accuracy: 0.6689 - f1_m: 0.7299 - precision_m: 0.6431 - recall_m: 0.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6895248889923096,\n",
       " 0.6689189076423645,\n",
       " 0.7298630475997925,\n",
       " 0.6430768966674805,\n",
       " 0.8507143259048462]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dd2be",
   "metadata": {},
   "source": [
    "loss: 0.6196 - binary_accuracy: 0.6892 - f1_m: 0.7617 - precision_m: 0.6551 - recall_m: 0.9233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b010ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "\n",
    "    # Count number of postive and negative bags.\n",
    "    negative_count = len(np.where(labels == 0)[0])\n",
    "    positive_count = len(np.where(labels == 1)[0])\n",
    "    total_count = negative_count + positive_count\n",
    "\n",
    "    # Build class weight dictionary.\n",
    "    return {\n",
    "        0: (1 / negative_count) * (total_count / 2),\n",
    "        1: (1 / positive_count) * (total_count / 2),\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
